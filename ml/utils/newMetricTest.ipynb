{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619771ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16.64595, 17.64888), (24.368349, 25.349689), (42.17569, 42.778351), (43.80769, 44.47702), (41.309021, 42.06636), (51.514622, 52.214931), (61.05003, 61.72472), (75.898003, 77.025993), (84.827797, 85.286003), (109.514198, 110.776497), (116.905998, 117.618103), (128.7603, 129.434601), (131.129303, 131.864105), (134.044998, 134.736801), (158.687897, 159.206604), (167.8526, 168.423004), (198.684097, 199.358398), (203.811203, 204.407898), (252.985703, 253.841599), (49.790119, 50.428871), (157.111206, 157.822998), (158.680695, 159.958206), (160.797699, 161.746704), (172.183502, 172.779694), (186.304596, 187.077194), (187.624695, 188.518906), (189.139404, 189.723404), (222.542496, 223.023102), (254.650299, 255.349899), (279.752411, 280.257294)] \n",
      "      start_s  end_s label     score\n",
      "33      16.5   17.5  call  0.998833\n",
      "34      17.0   18.0  call  0.982521\n",
      "48      24.0   25.0  call  0.996545\n",
      "49      24.5   25.5  call  0.998436\n",
      "82      41.0   42.0  call  0.990721\n",
      "..       ...    ...   ...       ...\n",
      "507    253.5  254.5  call  0.874752\n",
      "509    254.5  255.5  call  0.999393\n",
      "558    279.0  280.0  call  0.971597\n",
      "559    279.5  280.5  call  0.997097\n",
      "560    280.0  281.0  call  0.514338\n",
      "\n",
      "[67 rows x 4 columns]\n",
      "\n",
      "Metrics (Modified Evaluation Logic):\n",
      "Precision: 0.4242\n",
      "Recall: 0.9333\n",
      "F1-score: 0.5833\n",
      "AP50: 0.4306\n",
      "AP75: 0.0215\n",
      "AP50-95: 0.1439\n",
      "\n",
      "Bootstrap Confidence Intervals (Modified Evaluation Logic):\n",
      "Precision_CI: [0.1666 0.3667]\n",
      "Recall_CI: [0.3667 0.8008]\n",
      "F1-score_CI: [0.2268 0.5001]\n",
      "AP50_CI: [0.0536 0.3036]\n",
      "AP75_CI: [0.0000 0.0512]\n",
      "AP50-95_CI: [0.0149 0.0958]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm # Removed tqdm import\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "\n",
    "    # Сортировка по начальному времени\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    merged = [intervals[0]]\n",
    "\n",
    "    for current in intervals[1:]:\n",
    "        prev_start, prev_end = merged[-1]\n",
    "        curr_start, curr_end = current\n",
    "\n",
    "        # Проверка на перекрытие или соприкосновение\n",
    "        if curr_start <= prev_end:\n",
    "            # Объединяем интервалы\n",
    "            merged[-1] = (prev_start, max(prev_end, curr_end))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def merge_intervals_with_gap(intervals, gap_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Merges overlapping or closely spaced intervals based on a gap threshold.\n",
    "\n",
    "    Args:\n",
    "        intervals: A list of intervals as tuples (start, end).\n",
    "        gap_threshold: The maximum allowed gap between intervals to be merged.\n",
    "\n",
    "    Returns:\n",
    "        A list of merged intervals.\n",
    "    \"\"\"\n",
    "    if not intervals:\n",
    "        return []\n",
    "\n",
    "    # Sort intervals by start time\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "    merged = [intervals[0]]\n",
    "\n",
    "    for current_start, current_end in intervals[1:]:\n",
    "        prev_start, prev_end = merged[-1]\n",
    "\n",
    "        # Check for overlap or if the gap is within the threshold\n",
    "        if current_start <= prev_end or (current_start - prev_end) <= gap_threshold:\n",
    "            # Merge intervals\n",
    "            merged[-1] = (prev_start, max(prev_end, current_end))\n",
    "        else:\n",
    "            # Append as a new interval\n",
    "            merged.append((current_start, current_end))\n",
    "\n",
    "    return merged\n",
    "\n",
    "def calculate_iou(interval_a, interval_b):\n",
    "    start_a, end_a = interval_a\n",
    "    start_b, end_b = interval_b\n",
    "\n",
    "    intersection = max(0, min(end_a, end_b) - max(start_a, start_b))\n",
    "    union = (end_a - start_a) + (end_b - start_b) - intersection\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def calculate_ap(true_segments, pred_segments, iou_thresholds):\n",
    "    aps = []\n",
    "\n",
    "    for thresh in iou_thresholds:\n",
    "        # Стандартный расчет TP, FP для AP\n",
    "        # AP метрики обычно основаны на сопоставлении один-к-одному или один-ко-многим\n",
    "        # с учетом ранжирования предсказаний (по уверенности), которого у нас нет.\n",
    "        # Здесь используется стандартный расчет TP/FP для AP для каждого порога IoU.\n",
    "        tp_ap = np.zeros(len(pred_segments))\n",
    "        fp_ap = np.zeros(len(pred_segments))\n",
    "        matched_true_indices_ap = set()\n",
    "\n",
    "        for i, pred in enumerate(pred_segments):\n",
    "            best_iou = 0.0\n",
    "            best_true_idx = -1\n",
    "            for j, true in enumerate(true_segments):\n",
    "                # При стандартном подходе истинный интервал может быть сопоставлен только один раз\n",
    "                if j in matched_true_indices_ap:\n",
    "                    continue\n",
    "                iou = calculate_iou(pred, true)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_true_idx = j\n",
    "\n",
    "            if best_iou >= thresh:\n",
    "                matched_true_indices_ap.add(best_true_idx)\n",
    "                tp_ap[i] = 1\n",
    "            else:\n",
    "                fp_ap[i] = 1\n",
    "\n",
    "        # Рассчитываем precision-recall кривую для стандартного AP\n",
    "        tp_cumsum_ap = np.cumsum(tp_ap)\n",
    "        fp_cumsum_ap = np.cumsum(fp_ap)\n",
    "\n",
    "        recalls_ap = tp_cumsum_ap / len(true_segments) if len(true_segments) > 0 else np.zeros_like(tp_cumsum_ap)\n",
    "        precisions_ap = tp_cumsum_ap / (tp_cumsum_ap + fp_cumsum_ap + 1e-12)\n",
    "\n",
    "        # Интерполяция precision для 101 точки для стандартного AP\n",
    "        interp_precision_ap = np.zeros(101)\n",
    "        for r in range(101):\n",
    "            precision_vals_ap = precisions_ap[recalls_ap >= r/100]\n",
    "            interp_precision_ap[r] = max(precision_vals_ap) if len(precision_vals_ap) > 0 else 0\n",
    "\n",
    "        # Вычисляем AP как среднее значение precision для стандартного AP\n",
    "        ap_score = np.mean(interp_precision_ap)\n",
    "        aps.append(ap_score)\n",
    "\n",
    "    return aps\n",
    "\n",
    "\n",
    "def evaluate_metrics_modified(true_segments, pred_segments, iou_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Evaluates metrics allowing one predicted interval to match multiple true intervals.\n",
    "\n",
    "    Args:\n",
    "        true_segments: List of true intervals (start, end).\n",
    "        pred_segments: List of predicted intervals (start, end).\n",
    "        iou_threshold: IoU threshold for considering a match.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of calculated metrics (Precision, Recall, F1-score, AP50, AP75, AP50-95).\n",
    "    \"\"\"\n",
    "    # Новый расчет TP, FP, FN для Precision/Recall/F1\n",
    "    # Истинный позитив: истинный интервал, который пересекается с хотя бы одним предсказанным выше порога.\n",
    "    # Ложный позитив: предсказанный интервал, который не пересекается ни с одним истинным выше порога.\n",
    "    # Ложный негатив: истинный интервал, который не пересекается ни с одним предсказанным выше порога.\n",
    "\n",
    "    detected_true_count = 0\n",
    "    matched_pred_indices_for_fn_fp = set() # Для отслеживания предсказаний, которые совпали с истинными\n",
    "\n",
    "    for true_interval in true_segments:\n",
    "        is_true_detected = False\n",
    "        for pred_idx, pred_interval in enumerate(pred_segments):\n",
    "            if calculate_iou(true_interval, pred_interval) >= iou_threshold:\n",
    "                is_true_detected = True\n",
    "                matched_pred_indices_for_fn_fp.add(pred_idx)\n",
    "                # Не прерываем, так как один истинный может пересекаться с несколькими предсказанными\n",
    "        if is_true_detected:\n",
    "            detected_true_count += 1\n",
    "\n",
    "    true_positives = detected_true_count\n",
    "    false_negatives = len(true_segments) - true_positives\n",
    "\n",
    "    # Предсказанные интервалы, которые не совпали ни с одним истинным, являются ложными позитивами\n",
    "    false_positives = 0\n",
    "    for pred_idx in range(len(pred_segments)):\n",
    "        if pred_idx not in matched_pred_indices_for_fn_fp:\n",
    "            false_positives += 1\n",
    "\n",
    "    #print(f\"TP: {true_positives}, FN: {false_negatives}, FP: {false_positives}\")\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # AP metrics remain based on the standard one-to-one matching for PR curve calculation\n",
    "    ap50 = calculate_ap(true_segments, pred_segments, [0.5])[0]\n",
    "    ap75 = calculate_ap(true_segments, pred_segments, [0.75])[0]\n",
    "    ap50_95 = np.mean(calculate_ap(true_segments, pred_segments, np.arange(0.5, 1.0, 0.05)))\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1_score,\n",
    "        'AP50': ap50,\n",
    "        'AP75': ap75,\n",
    "        'AP50-95': ap50_95\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_confidence_intervals_modified(true_segments, pred_segments, iou_threshold=0.2,\n",
    "                                  n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculates bootstrap confidence intervals using the modified evaluation metrics.\n",
    "    \"\"\"\n",
    "    n_true = len(true_segments)\n",
    "    n_pred = len(pred_segments)\n",
    "\n",
    "    metrics_samples = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-score': [],\n",
    "        'AP50': [],\n",
    "        'AP75': [],\n",
    "        'AP50-95': []\n",
    "    }\n",
    "\n",
    "    for _ in range(n_bootstrap): # Removed tqdm\n",
    "        # Bootstrap с заменой из исходных выборок\n",
    "        true_bootstrap_indices = np.random.choice(n_true, n_true, replace=True)\n",
    "        pred_bootstrap_indices = np.random.choice(n_pred, n_pred, replace=True)\n",
    "\n",
    "        true_bootstrap = [true_segments[i] for i in true_bootstrap_indices]\n",
    "        pred_bootstrap = [pred_segments[i] for i in pred_bootstrap_indices]\n",
    "\n",
    "        # Используем модифицированную функцию оценки метрик\n",
    "        metrics = evaluate_metrics_modified(true_bootstrap, pred_bootstrap, iou_threshold)\n",
    "        for k in metrics_samples:\n",
    "            metrics_samples[k].append(metrics[k])\n",
    "\n",
    "    # Вычисление квантилей для всех метрик\n",
    "    alpha = (1 - confidence_level) / 2\n",
    "    ci = {}\n",
    "\n",
    "    for metric, samples in metrics_samples.items():\n",
    "        ci[metric + \"_CI\"] = np.percentile(samples, [alpha*100, (1-alpha)*100])\n",
    "\n",
    "    return ci\n",
    "\n",
    "# Предполагается, что detections и true_label.txt уже загружены и обработаны\n",
    "# как в исходном коде для получения true_intervals и predicted_intervals (после merge_intervals, если используете его)\n",
    "\n",
    "# Пример использования с модифицированной функцией:\n",
    "\n",
    "# Загрузка данных (как в исходном коде)\n",
    "try:\n",
    "    detections = pd.read_csv('../dataset/detections.csv')\n",
    "    detections_call = detections[detections['label'] == 'call']\n",
    "    # Использование оригинальной merge_intervals или новой merge_intervals_with_gap\n",
    "    # Если вы хотите использовать объединенные интервалы для предсказаний:\n",
    "    # predicted_intervals_for_eval = merge_intervals_with_gap(list(zip(detections_call['start_s'], detections_call['end_s']))) # Используем merge_intervals_with_gap\n",
    "    # Если вы хотите использовать необъединенные интервалы для предсказаний:\n",
    "    predicted_intervals_for_eval = (list(zip(detections_call['start_s'], detections_call['end_s'])))\n",
    "\n",
    "\n",
    "    true_intervals = []\n",
    "    with open('../dataset/true_label.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            start_str = parts[0].replace(',', '.')\n",
    "            end_str = parts[1].replace(',', '.')\n",
    "            start = float(start_str)\n",
    "            end = float(end_str)\n",
    "            true_intervals.append((start, end))\n",
    "\n",
    "    print(true_intervals, \"\\n\" ,detections_call)\n",
    "    # IoU порог для оценки (можно настроить)\n",
    "    iou_global = 0.5\n",
    "\n",
    "    # Оценка метрик с модифицированной функцией\n",
    "    metrics_modified = evaluate_metrics_modified(true_intervals, predicted_intervals_for_eval, iou_threshold=iou_global)\n",
    "\n",
    "    print(\"\\nMetrics (Modified Evaluation Logic):\")\n",
    "    for metric, value in metrics_modified.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Расчет бутстрап доверительных интервалов с модифицированной функцией оценки\n",
    "    # Установите n_bootstrap на меньшее число, например 100, для более быстрого выполнения, если необходимо.\n",
    "    bootstrap_ci_modified = bootstrap_confidence_intervals_modified(true_intervals, predicted_intervals_for_eval, iou_threshold=iou_global, n_bootstrap=1000)\n",
    "\n",
    "    print(\"\\nBootstrap Confidence Intervals (Modified Evaluation Logic):\")\n",
    "    for metric, ci_values in bootstrap_ci_modified.items():\n",
    "        print(f\"{metric}: [{ci_values[0]:.4f} {ci_values[1]:.4f}]\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: dataset files not found. Please make sure 'detections.csv' and 'true_label.txt' are in the '../dataset/' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
