{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Генерация фальшивых данных\n",
    "def generate_fake_data(num_samples=1000):\n",
    "    np.random.seed(42)\n",
    "    y_true = np.random.randint(0, 2, size=num_samples)  # Истинные значения (0 или 1)\n",
    "    y_pred = np.random.randint(0, 2, size=num_samples)  # Предсказанные значения (0 или 1)\n",
    "    \n",
    "    # Создание DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'true_label': y_true,\n",
    "        'predicted_label': y_pred\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "fake_data = generate_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9091\n",
      "Recall: 1.0000\n",
      "F1-score: 0.9524\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(interval_a, interval_b):\n",
    "    start_a, end_a = interval_a\n",
    "    start_b, end_b = interval_b\n",
    "    \n",
    "    intersection = max(0, min(end_a, end_b) - max(start_a, start_b))\n",
    "    union = (end_a - start_a) + (end_b - start_b) - intersection\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def evaluate_metrics(true_segments, pred_segments, iou_threshold=0.5): #Важный коэф надо покрутить\n",
    "    true_positives = 0\n",
    "    matched_true_indices = set()\n",
    "    \n",
    "    for pred in pred_segments:\n",
    "        best_iou = 0.0\n",
    "        best_true_idx = -1\n",
    "        \n",
    "        for i, true in enumerate(true_segments):\n",
    "            if i in matched_true_indices:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(pred, true)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_true_idx = i\n",
    "                \n",
    "        if best_iou >= iou_threshold:\n",
    "            true_positives += 1\n",
    "            matched_true_indices.add(best_true_idx)\n",
    "    \n",
    "    false_positives = len(pred_segments) - true_positives\n",
    "    false_negatives = len(true_segments) - true_positives\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1_score,\n",
    "    }\n",
    "\n",
    "# Пример использования\n",
    "true_intervals = [\n",
    "    (0, 5.2), (7.1, 12.3), (15.0, 18.9), \n",
    "    (22.4, 27.8), (30.1, 35.5), (38.0, 43.2),\n",
    "    (47.5, 52.0), (55.8, 60.3), (63.7, 68.9),\n",
    "    (72.0, 77.4)\n",
    "]\n",
    "predicted_intervals = [\n",
    "    (0.1, 5.3), (6.9, 12.1), (14.8, 18.7), \n",
    "    (22.0, 27.0), (29.9, 34.0), (37.5, 42.0), \n",
    "    (47.0, 51.5), (55.0, 59.8), (63.0, 67.5),\n",
    "    (71.5, 76.0), (80.0, 85.2) \n",
    "]\n",
    "\n",
    "metrics = evaluate_metrics(true_intervals, predicted_intervals)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping: 100%|██████████| 1000/1000 [00:00<00:00, 8000.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Доверительные интервалы (95%):\n",
      "Precision: [0.18181818 0.72727273]\n",
      "Recall: [0.2 0.8]\n",
      "F1-score: [0.19047619 0.76190476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_confidence_intervals(true_segments, pred_segments, iou_threshold=0.5, \n",
    "                                 n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"Вычисляет доверительные интервалы метрик с помощью бутстрепа.\n",
    "        true_segments: Эталонные интервалы\n",
    "        pred_segments: Предсказанные интервалы\n",
    "        iou_threshold: Порог IoU\n",
    "        n_bootstrap: Количество бутстреп-выборок (ставьте поменьше для большой выборки)\n",
    "        confidence_level: Уровень доверия (0.95 для 95%)\n",
    "    \"\"\"\n",
    "    # Исходные метрики\n",
    "\n",
    "    # Бутстреп-выборки\n",
    "    n_true = len(true_segments)\n",
    "    n_pred = len(pred_segments)\n",
    "    \n",
    "    precision_samples = []\n",
    "    recall_samples = []\n",
    "    f1_samples = []\n",
    "    \n",
    "    for _ in tqdm(range(n_bootstrap), desc=\"Bootstrapping\"):\n",
    "        # Генерация бутстреп-выборок с повторением\n",
    "        true_bootstrap = [true_segments[i] for i in np.random.choice(n_true, n_true, replace=True)]\n",
    "        pred_bootstrap = [pred_segments[i] for i in np.random.choice(n_pred, n_pred, replace=True)]\n",
    "        \n",
    "        # Вычисление метрик для выборки\n",
    "        metrics = evaluate_metrics(true_bootstrap, pred_bootstrap, iou_threshold)\n",
    "        precision_samples.append(metrics['Precision'])\n",
    "        recall_samples.append(metrics['Recall'])\n",
    "        f1_samples.append(metrics['F1-score'])\n",
    "    \n",
    "    # Вычисление квантилей\n",
    "    alpha = (1 - confidence_level) / 2\n",
    "    ci_low = alpha * 100\n",
    "    ci_high = (1 - alpha) * 100\n",
    "    \n",
    "    def get_ci(samples):\n",
    "        return np.percentile(samples, [ci_low, ci_high])\n",
    "    \n",
    "    return {\n",
    "        'Precision_CI': get_ci(precision_samples),\n",
    "        'Recall_CI': get_ci(recall_samples),\n",
    "        'F1_CI': get_ci(f1_samples),\n",
    "    }\n",
    "\n",
    "bootstrap = bootstrap_confidence_intervals(true_intervals, predicted_intervals)\n",
    "print(\"\\nДоверительные интервалы (95%):\")\n",
    "print(f\"Precision: {bootstrap['Precision_CI']}\")\n",
    "print(f\"Recall: {bootstrap['Recall_CI']}\")\n",
    "print(f\"F1-score: {bootstrap['F1_CI']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
