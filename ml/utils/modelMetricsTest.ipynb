{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Генерация фальшивых данных\n",
    "def generate_fake_data(num_samples=1000):\n",
    "    np.random.seed(42)\n",
    "    y_true = np.random.randint(0, 2, size=num_samples)  # Истинные значения (0 или 1)\n",
    "    y_pred = np.random.randint(0, 2, size=num_samples)  # Предсказанные значения (0 или 1)\n",
    "    \n",
    "    # Создание DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'true_label': y_true,\n",
    "        'predicted_label': y_pred\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "fake_data = generate_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8182\n",
      "Recall: 0.9000\n",
      "F1-score: 0.8571\n",
      "AP50: 0.8416\n",
      "AP75: 0.4307\n",
      "AP50-95: 0.5248\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(interval_a, interval_b):\n",
    "    start_a, end_a = interval_a\n",
    "    start_b, end_b = interval_b\n",
    "    \n",
    "    intersection = max(0, min(end_a, end_b) - max(start_a, start_b))\n",
    "    union = (end_a - start_a) + (end_b - start_b) - intersection\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def calculate_ap(true_segments, pred_segments, iou_thresholds):\n",
    "    aps = []\n",
    "    \n",
    "    for thresh in iou_thresholds:\n",
    "        # Сопоставление предсказаний с истинными интервалами\n",
    "        matched_true = set()\n",
    "        tp = np.zeros(len(pred_segments))\n",
    "        fp = np.zeros(len(pred_segments))\n",
    "        \n",
    "        for i, pred in enumerate(pred_segments):\n",
    "            best_iou = 0.0\n",
    "            best_idx = -1\n",
    "            for j, true in enumerate(true_segments):\n",
    "                if j in matched_true:\n",
    "                    continue\n",
    "                iou = calculate_iou(pred, true)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_idx = j\n",
    "            \n",
    "            if best_iou >= thresh:\n",
    "                matched_true.add(best_idx)\n",
    "                tp[i] = 1\n",
    "            else:\n",
    "                fp[i] = 1\n",
    "\n",
    "        # Рассчитываем precision-recall кривую\n",
    "        tp_cumsum = np.cumsum(tp)\n",
    "        fp_cumsum = np.cumsum(fp)\n",
    "        \n",
    "        recalls = tp_cumsum / len(true_segments)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-12)\n",
    "        \n",
    "        # Интерполяция precision для 101 точки\n",
    "        interp_precision = np.zeros(101)\n",
    "        for r in range(101):\n",
    "            precision_vals = precisions[recalls >= r/100]\n",
    "            interp_precision[r] = max(precision_vals) if len(precision_vals) > 0 else 0\n",
    "        \n",
    "        # Вычисляем AP как среднее значение precision\n",
    "        ap = np.mean(interp_precision)\n",
    "        aps.append(ap)\n",
    "    \n",
    "    return aps\n",
    "\n",
    "def evaluate_metrics(true_segments, pred_segments, iou_threshold=0.5): #Важный коэф надо покрутить\n",
    "    true_positives = 0\n",
    "    matched_true_indices = set()\n",
    "    \n",
    "    for pred in pred_segments:\n",
    "        best_iou = 0.0\n",
    "        best_true_idx = -1\n",
    "        \n",
    "        for i, true in enumerate(true_segments):\n",
    "            if i in matched_true_indices:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(pred, true)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_true_idx = i\n",
    "                \n",
    "        if best_iou >= iou_threshold:\n",
    "            true_positives += 1\n",
    "            matched_true_indices.add(best_true_idx)\n",
    "    \n",
    "    false_positives = len(pred_segments) - true_positives\n",
    "    false_negatives = len(true_segments) - true_positives\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1_score,\n",
    "        'AP50': calculate_ap(true_segments, pred_segments, [0.5])[0],\n",
    "        'AP75': calculate_ap(true_segments, pred_segments, [0.75])[0],\n",
    "        'AP50-95': np.mean(calculate_ap(true_segments, pred_segments, np.arange(0.5, 1.0, 0.05)))\n",
    "    }\n",
    "\n",
    "# Пример использования\n",
    "true_intervals = [\n",
    "    (0, 5.2), (7.1, 12.3), (15.0, 18.9), \n",
    "    (22.4, 23.8), (30.1, 35.5), (38.0, 43.2),\n",
    "    (47.5, 52.0), (55.8, 60.3), (63.7, 68.9),\n",
    "    (72.0, 77.4)\n",
    "]\n",
    "predicted_intervals = [\n",
    "    (0.1, 5.3), (6.9, 12.1), (14.8, 18.7), \n",
    "    (22.6, 27.0), (29.9, 34.0), (37.5, 42.0), \n",
    "    (47.0, 51.5), (55.0, 59.8), (63.0, 67.5),\n",
    "    (71.5, 76.0), (80.0, 85.2)  \n",
    "]\n",
    "\n",
    "metrics = evaluate_metrics(true_intervals, predicted_intervals)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping: 100%|██████████| 1000/1000 [00:04<00:00, 242.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision_CI': array([0.18181818, 0.63636364]), 'Recall_CI': array([0.2, 0.7]), 'F1-score_CI': array([0.19047619, 0.66666667]), 'AP50_CI': array([0.09023777, 0.63462346]), 'AP75_CI': array([0.00990099, 0.42942044]), 'AP50-95_CI': array([0.04562331, 0.38684635])}\n",
      "Precision_CI: [0.18181818 0.63636364]\n",
      "Recall_CI: [0.2 0.7]\n",
      "F1-score_CI: [0.19047619 0.66666667]\n",
      "AP50_CI: [0.09023777 0.63462346]\n",
      "AP75_CI: [0.00990099 0.42942044]\n",
      "AP50-95_CI: [0.04562331 0.38684635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_confidence_intervals(true_segments, pred_segments, iou_threshold=0.5,\n",
    "                                  n_bootstrap=1000, confidence_level=0.95):\n",
    "    # Модифицируем для новых метрик\n",
    "    n_true = len(true_segments)\n",
    "    n_pred = len(pred_segments)\n",
    "    \n",
    "    metrics_samples = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-score': [],\n",
    "        'AP50': [],\n",
    "        'AP75': [],\n",
    "        'AP50-95': []\n",
    "    }\n",
    "    \n",
    "    for _ in tqdm(range(n_bootstrap), desc=\"Bootstrapping\"):\n",
    "        true_bootstrap = [true_segments[i] for i in np.random.choice(n_true, n_true, replace=True)]\n",
    "        pred_bootstrap = [pred_segments[i] for i in np.random.choice(n_pred, n_pred, replace=True)]\n",
    "        \n",
    "        metrics = evaluate_metrics(true_bootstrap, pred_bootstrap, iou_threshold)\n",
    "        for k in metrics_samples:\n",
    "            metrics_samples[k].append(metrics[k])\n",
    "    \n",
    "    # Вычисление квантилей для всех метрик\n",
    "    alpha = (1 - confidence_level) / 2\n",
    "    ci = {}\n",
    "    \n",
    "    for metric, samples in metrics_samples.items():\n",
    "        ci[metric + \"_CI\"] = np.percentile(samples, [alpha*100, (1-alpha)*100])\n",
    "    \n",
    "    return ci\n",
    "\n",
    "bootstrap = bootstrap_confidence_intervals(true_intervals, predicted_intervals)\n",
    "print(bootstrap)\n",
    "for metric in bootstrap:\n",
    "    print(f\"{metric}: {bootstrap[metric]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
